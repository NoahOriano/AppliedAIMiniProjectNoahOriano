{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noaho\\OneDrive\\Desktop\\Applied AI\\MiniProjectNoahOriano\\Evaluate.py:192: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_x[\"Sex\"] = data_x[\"Sex\"].map({'I': 0, 'F': 1, 'M': 2})\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\neighbors\\_classification.py:238: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: SOM_Test: class of sample is greater than the number of classes\n",
      "AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    }
   ],
   "source": [
    "# Execute the evaluation of all models on all datasets in evaluate.py\n",
    "# This will store the confusion matrices of all the datasets and models in the results folder\n",
    "\n",
    "import Evaluate\n",
    "Evaluate.evaulate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper import load_results\n",
    "from Helper import is_evaluated\n",
    "from Helper import calculate_accuracy\n",
    "\n",
    "def get_model_accuracies(models, datasets):\n",
    "    accuracies = {}\n",
    "    for model in models:\n",
    "        accuracies[model] = {}\n",
    "        for dataset in datasets:\n",
    "            if is_evaluated(model, dataset):\n",
    "                cm = load_results(model, dataset)\n",
    "                accuracy = calculate_accuracy(cm)\n",
    "                accuracies[model][dataset] = accuracy\n",
    "    return accuracies\n",
    "\n",
    "def display_model_accuracies(accuracies):\n",
    "    for model, datasets in accuracies.items():\n",
    "        print(f\"Model: {model}\")\n",
    "        for dataset, accuracy in datasets.items():\n",
    "            print(f\"Dataset: {dataset}, Accuracy: {accuracy}\")\n",
    "        print()\n",
    "\n",
    "def display_model_accuracies_and_scores(models, datasets):\n",
    "    scores = {}\n",
    "    for model in models:\n",
    "        scores[model] = {}\n",
    "        for dataset in datasets:\n",
    "            cm = load_results(model, dataset)\n",
    "            # Find the number of classes\n",
    "            num_classes = len(cm)\n",
    "            # Calculate the accuracy\n",
    "            accuracy = calculate_accuracy(cm)\n",
    "            # Determine the precision, recall and f1 score for each class\n",
    "            precision = []\n",
    "            recall = []\n",
    "            f1 = []\n",
    "            for i in range(num_classes):\n",
    "                tp = cm[i][i]\n",
    "                fp = sum([cm[j][i] for j in range(num_classes)]) - tp\n",
    "                fn = sum(cm[i]) - tp\n",
    "                precision.append(tp / (tp + fp + 0.0001))\n",
    "                recall.append(tp / (tp + fn + 0.0001))\n",
    "                f1.append(2 * precision[i] * recall[i] / (precision[i] + recall[i] + 0.0001))\n",
    "            # Calculate the macro average of precision, recall and f1 score\n",
    "            macro_precision = sum(precision) / num_classes\n",
    "            macro_recall = sum(recall) / num_classes\n",
    "            macro_f1 = sum(f1) / num_classes\n",
    "            scores[model][dataset] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"macro_precision\": macro_precision,\n",
    "                \"macro_recall\": macro_recall,\n",
    "                \"macro_f1\": macro_f1,\n",
    "                \"classes\": num_classes,\n",
    "                \"data_count\": sum([sum(row) for row in cm])  # Calculate the total number of data points\n",
    "            }\n",
    "        scores[model][\"average\"] = {\n",
    "            \"accuracy\": sum([scores[model][dataset][\"accuracy\"] for dataset in datasets]) / len(datasets),\n",
    "            \"macro_precision\": sum([scores[model][dataset][\"macro_precision\"] for dataset in datasets]) / len(datasets),\n",
    "            \"macro_recall\": sum([scores[model][dataset][\"macro_recall\"] for dataset in datasets]) / len(datasets),\n",
    "            \"macro_f1\": sum([scores[model][dataset][\"macro_f1\"] for dataset in datasets]) / len(datasets),\n",
    "            \"classes\": sum([scores[model][dataset][\"classes\"] for dataset in datasets]) / len(datasets),\n",
    "            \"data_count\": sum([scores[model][dataset][\"data_count\"] for dataset in datasets]) / len(datasets)\n",
    "        }\n",
    "    # Display the scores in a table using the tabulate library\n",
    "    from tabulate import tabulate\n",
    "    for model in scores:\n",
    "        print(f\"Model: {model}\")\n",
    "        headers = [\"Dataset\", \"Classes\", \"Test Data\", \"Accuracy\", \"Macro Precision\", \"Macro Recall\", \"Macro F1\"]\n",
    "        table = []\n",
    "        for dataset in datasets:\n",
    "            table.append([\n",
    "                dataset,\n",
    "                scores[model][dataset][\"classes\"],\n",
    "                scores[model][dataset][\"data_count\"],\n",
    "                scores[model][dataset][\"accuracy\"],\n",
    "                scores[model][dataset][\"macro_precision\"],\n",
    "                scores[model][dataset][\"macro_recall\"],\n",
    "                scores[model][dataset][\"macro_f1\"]\n",
    "            ])\n",
    "        table.append([\n",
    "            \"Average\",\n",
    "            scores[model][\"average\"][\"classes\"],\n",
    "            scores[model][\"average\"][\"data_count\"],\n",
    "            scores[model][\"average\"][\"accuracy\"],\n",
    "            scores[model][\"average\"][\"macro_precision\"],\n",
    "            scores[model][\"average\"][\"macro_recall\"],\n",
    "            scores[model][\"average\"][\"macro_f1\"]\n",
    "        ])\n",
    "        print(tabulate(table, headers, tablefmt=\"grid\"))\n",
    "        print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: som\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| Dataset   |   Classes |   Test Data |   Accuracy |   Macro Precision |   Macro Recall |   Macro F1 |\n",
      "+===========+===========+=============+============+===================+================+============+\n",
      "| cifar10   |        10 |        2000 |     0.3345 |          0.336629 |       0.331349 |   0.324809 |\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| mnist     |        10 |        2000 |     0.8585 |          0.860624 |       0.8551   |   0.856    |\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| Average   |        10 |        2000 |     0.5965 |          0.598627 |       0.593224 |   0.590405 |\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "\n",
      "Model: rf\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| Dataset             |   Classes |   Test Data |   Accuracy |   Macro Precision |   Macro Recall |   Macro F1 |\n",
      "+=====================+===========+=============+============+===================+================+============+\n",
      "| abalone             |   21      |      836    |   0.263158 |          0.190899 |       0.173284 |   0.172379 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| breast_cancer       |    2      |      114    |   0.964912 |          0.967255 |       0.958072 |   0.96225  |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| california_housing  |   15      |     4128    |   0.522529 |          0.353257 |       0.338529 |   0.34414  |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| cifar10             |   10      |     2000    |   0.4215   |          0.416415 |       0.41962  |   0.415003 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| cifar100            |  100      |     2000    |   0.1505   |          0.153474 |       0.153971 |   0.133005 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| digits              |   10      |      360    |   0.972222 |          0.97404  |       0.972698 |   0.973154 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| dry_beans           |    6      |     2000    |   0.9565   |          0.960343 |       0.958572 |   0.959346 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| eurosat             |   10      |     2000    |   0.639    |          0.619161 |       0.63187  |   0.619857 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| fashion_mnist       |   10      |     2000    |   0.852    |          0.853044 |       0.855187 |   0.85289  |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| iris                |    3      |       30    |   1        |          0.99999  |       0.99999  |   0.99994  |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| malaria             |    2      |     2000    |   0.792    |          0.792573 |       0.791955 |   0.79183  |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| mnist               |   10      |     2000    |   0.95     |          0.94919  |       0.948593 |   0.948688 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| mnist_corrupted     |   10      |     2000    |   0.9425   |          0.942418 |       0.942479 |   0.942335 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| penguins            |    3      |       69    |   0.956522 |          0.953258 |       0.958329 |   0.955487 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| rice                |    2      |      762    |   0.925197 |          0.92495  |       0.924376 |   0.924598 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| rock_paper_scissors |    3      |      504    |   0.996032 |          0.995909 |       0.99619  |   0.995976 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| skin_segmentation   |    2      |     2000    |   0.9975   |          0.994675 |       0.99749  |   0.996025 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| tf_flowers          |    5      |      734    |   0.527248 |          0.533706 |       0.513357 |   0.515694 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| titanic             |    2      |      143    |   0.776224 |          0.765188 |       0.765188 |   0.765138 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| wine                |    3      |       36    |   1        |          0.999991 |       0.999991 |   0.999941 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| wine_quality        |    7      |     1300    |   0.667692 |          0.523104 |       0.35135  |   0.384768 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| Average             |   11.2381 |     1286.48 |   0.774916 |          0.755373 |       0.74529  |   0.745355 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "\n",
      "Model: rf, Average Accuracy: 0.7749159925382617\n",
      "Model: dt, Average Accuracy: 0.6811914899295864\n",
      "Model: ann, Average Accuracy: 0.7426801148898707\n",
      "Model: svm, Average Accuracy: 0.7176986536168641\n",
      "Model: knn, Average Accuracy: 0.7028877273890357\n",
      "Model: som, Average Accuracy: 0.742240760566981\n",
      "Model: rf, Average Image Accuracy: 0.7365866336038905, Average Tabular Accuracy: 0.8036630117390401\n",
      "Model: dt, Average Image Accuracy: 0.6280792857863704, Average Tabular Accuracy: 0.7246469296831268\n",
      "Model: ann, Average Image Accuracy: 0.7073460490463215, Average Tabular Accuracy: 0.7715898051255015\n",
      "Model: svm, Average Image Accuracy: 0.6706110053871179, Average Tabular Accuracy: 0.7530143897891737\n",
      "Model: knn, Average Image Accuracy: 0.6584611200015379, Average Tabular Accuracy: 0.7392367697969885\n",
      "Model: som, Average Image Accuracy: 0.6625227258528803, Average Tabular Accuracy: 0.8319235496203442\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/som/abalone.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, accuracy \u001b[38;5;129;01min\u001b[39;00m avearage_accuracies\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Image Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_image_accuracies[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Average Tabular Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_tabular_accuracies[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mdisplay_model_accuracies_and_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m, in \u001b[0;36mdisplay_model_accuracies_and_scores\u001b[1;34m(models, datasets)\u001b[0m\n\u001b[0;32m     26\u001b[0m scores[model] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m---> 28\u001b[0m     cm \u001b[38;5;241m=\u001b[39m \u001b[43mload_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Find the number of classes\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(cm)\n",
      "File \u001b[1;32mc:\\Users\\noaho\\OneDrive\\Desktop\\Applied AI\\MiniProjectNoahOriano\\Helper.py:10\u001b[0m, in \u001b[0;36mload_results\u001b[1;34m(model_name, dataset_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_results\u001b[39m(model_name, dataset_name):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\lib\\npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/som/abalone.npy'"
     ]
    }
   ],
   "source": [
    "models = [\"rf\"]\n",
    "# Print accuracy of som model on cifar10 dataset\n",
    "accuracies = display_model_accuracies_and_scores([\"som\"], [\"cifar10\", \"mnist\"])\n",
    "accuracies = display_model_accuracies_and_scores([\"gmm\"], [\"cifar10\", \"mnist\"])\n",
    "\n",
    "# Get all datasets by globbing the results/rf folder\n",
    "import glob\n",
    "datasets = [path.split(\"\\\\\")[-1] for path in glob.glob(\"results\\\\rf\\\\*\")]\n",
    "# Remove the .npy extension\n",
    "datasets = [dataset.split(\".\")[0] for dataset in datasets]\n",
    "\n",
    "image_datasets = [\"cifar10\", \"cifar100\", \"mnist\", \"fashion_mnist\", \"malaria\", \"mnist_corrupted\", \"rock_paper_scissors\", \"skin_segmentation\", \"tf_flowers\"]\n",
    "tabular_datasets = [dataset for dataset in datasets if dataset not in image_datasets]\n",
    "\n",
    "# Get the accuracies of the models\n",
    "accuracies = get_model_accuracies(models, datasets)\n",
    "display_model_accuracies_and_scores(models, datasets)\n",
    "\n",
    "# Get the accuracies of all the supervised models\n",
    "supervised_models = [\"rf\", \"dt\", \"ann\", \"svm\"]\n",
    "# Get the accuracies of all the unsupervised models\n",
    "unsupervised_models = [\"knn\", \"som\"]\n",
    "models = supervised_models + unsupervised_models\n",
    "accuracies = get_model_accuracies(models, datasets)\n",
    "# Print the average accuracy of each model\n",
    "avearage_accuracies = {model: sum(accuracies[model].values()) / (len(accuracies[model])) for model in accuracies}\n",
    "for model, accuracy in avearage_accuracies.items():\n",
    "    print(f\"Model: {model}, Average Accuracy: {accuracy}\")\n",
    "\n",
    "# Print the average accuracy of each model on image vs tabular datasets\n",
    "image_accuracies = get_model_accuracies(models, image_datasets)\n",
    "tabular_accuracies = get_model_accuracies(models, tabular_datasets)\n",
    "average_image_accuracies = {model: sum(image_accuracies[model].values()) / len(image_accuracies[model]) for model in image_accuracies}\n",
    "average_tabular_accuracies = {model: sum(tabular_accuracies[model].values()) / len(tabular_accuracies[model]) for model in tabular_accuracies}\n",
    "for model, accuracy in avearage_accuracies.items():\n",
    "    print(f\"Model: {model}, Average Image Accuracy: {average_image_accuracies[model]}, Average Tabular Accuracy: {average_tabular_accuracies[model]}\")\n",
    "\n",
    "display_model_accuracies_and_scores([\"som\"], datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
