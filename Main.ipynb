{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "c:\\Users\\noaho\\OneDrive\\Desktop\\Applied AI\\MiniProjectNoahOriano\\Evaluate.py:192: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_x[\"Sex\"] = data_x[\"Sex\"].map({'I': 0, 'F': 1, 'M': 2})\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttributeError: module 'ml_dtypes' has no attribute 'float8_e3m4'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n",
      "C:\\Users\\noaho\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\mixture\\_base.py:270: ConvergenceWarning: Best performing initialization did not converge. Try different init parameters, or increase max_iter, tol, or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Execute the evaluation of all models on all datasets in evaluate.py\n",
    "# This will store the confusion matrices of all the datasets and models in the results folder\n",
    "\n",
    "import Evaluate\n",
    "Evaluate.evaulate_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper import load_results\n",
    "from Helper import is_evaluated\n",
    "from Helper import calculate_accuracy\n",
    "\n",
    "def get_model_accuracies(models, datasets):\n",
    "    accuracies = {}\n",
    "    for model in models:\n",
    "        accuracies[model] = {}\n",
    "        for dataset in datasets:\n",
    "            if is_evaluated(model, dataset):\n",
    "                cm = load_results(model, dataset)\n",
    "                accuracy = calculate_accuracy(cm)\n",
    "                accuracies[model][dataset] = accuracy\n",
    "    return accuracies\n",
    "\n",
    "def get_model_scores(models, datasets):\n",
    "    # Get the f1 score for each model and dataset\n",
    "    scores = {}\n",
    "    for model in models:\n",
    "        scores[model] = {}\n",
    "        for dataset in datasets:\n",
    "            cm = load_results(model, dataset)\n",
    "            # Find the number of classes\n",
    "            num_classes = len(cm)\n",
    "            # Calculate the accuracy\n",
    "            accuracy = calculate_accuracy(cm)\n",
    "            # Determine the precision, recall and f1 score for each class\n",
    "            precision = []\n",
    "            recall = []\n",
    "            f1 = []\n",
    "            for i in range(num_classes):\n",
    "                tp = cm[i][i]\n",
    "                fp = sum([cm[j][i] for j in range(num_classes)]) - tp\n",
    "                fn = sum(cm[i]) - tp\n",
    "                precision.append(tp / (tp + fp + 0.0001))\n",
    "                recall.append(tp / (tp + fn + 0.0001))\n",
    "                f1.append(2 * precision[i] * recall[i] / (precision[i] + recall[i] + 0.0001))\n",
    "            # Calculate the macro average of f1 score\n",
    "            macro_f1 = sum(f1) / num_classes\n",
    "            scores[model][dataset] = macro_f1\n",
    "    return scores\n",
    "\n",
    "def display_model_accuracies(accuracies):\n",
    "    for model, datasets in accuracies.items():\n",
    "        print(f\"Model: {model}\")\n",
    "        for dataset, accuracy in datasets.items():\n",
    "            print(f\"Dataset: {dataset}, Accuracy: {accuracy}\")\n",
    "        print()\n",
    "\n",
    "def display_model_accuracies_and_scores(models, datasets):\n",
    "    scores = {}\n",
    "    for model in models:\n",
    "        scores[model] = {}\n",
    "        for dataset in datasets:\n",
    "            cm = load_results(model, dataset)\n",
    "            # Find the number of classes\n",
    "            num_classes = len(cm)\n",
    "            # Calculate the accuracy\n",
    "            accuracy = calculate_accuracy(cm)\n",
    "            # Determine the precision, recall and f1 score for each class\n",
    "            precision = []\n",
    "            recall = []\n",
    "            f1 = []\n",
    "            for i in range(num_classes):\n",
    "                tp = cm[i][i]\n",
    "                fp = sum([cm[j][i] for j in range(num_classes)]) - tp\n",
    "                fn = sum(cm[i]) - tp\n",
    "                precision.append(tp / (tp + fp + 0.0001))\n",
    "                recall.append(tp / (tp + fn + 0.0001))\n",
    "                f1.append(2 * precision[i] * recall[i] / (precision[i] + recall[i] + 0.0001))\n",
    "            # Calculate the macro average of precision, recall and f1 score\n",
    "            macro_precision = sum(precision) / num_classes\n",
    "            macro_recall = sum(recall) / num_classes\n",
    "            macro_f1 = sum(f1) / num_classes\n",
    "            scores[model][dataset] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f1\": f1,\n",
    "                \"macro_precision\": macro_precision,\n",
    "                \"macro_recall\": macro_recall,\n",
    "                \"macro_f1\": macro_f1,\n",
    "                \"classes\": num_classes,\n",
    "                \"data_count\": sum([sum(row) for row in cm])  # Calculate the total number of data points\n",
    "            }\n",
    "        scores[model][\"average\"] = {\n",
    "            \"accuracy\": sum([scores[model][dataset][\"accuracy\"] for dataset in datasets]) / len(datasets),\n",
    "            \"macro_precision\": sum([scores[model][dataset][\"macro_precision\"] for dataset in datasets]) / len(datasets),\n",
    "            \"macro_recall\": sum([scores[model][dataset][\"macro_recall\"] for dataset in datasets]) / len(datasets),\n",
    "            \"macro_f1\": sum([scores[model][dataset][\"macro_f1\"] for dataset in datasets]) / len(datasets),\n",
    "            \"classes\": sum([scores[model][dataset][\"classes\"] for dataset in datasets]) / len(datasets),\n",
    "            \"data_count\": sum([scores[model][dataset][\"data_count\"] for dataset in datasets]) / len(datasets)\n",
    "        }\n",
    "    # Display the scores in a table using the tabulate library\n",
    "    from tabulate import tabulate\n",
    "    for model in scores:\n",
    "        print(f\"Model: {model}\")\n",
    "        headers = [\"Dataset\", \"Classes\", \"Test Data\", \"Accuracy\", \"Macro Precision\", \"Macro Recall\", \"Macro F1\"]\n",
    "        table = []\n",
    "        for dataset in datasets:\n",
    "            table.append([\n",
    "                dataset,\n",
    "                scores[model][dataset][\"classes\"],\n",
    "                scores[model][dataset][\"data_count\"],\n",
    "                scores[model][dataset][\"accuracy\"],\n",
    "                scores[model][dataset][\"macro_precision\"],\n",
    "                scores[model][dataset][\"macro_recall\"],\n",
    "                scores[model][dataset][\"macro_f1\"]\n",
    "            ])\n",
    "        table.append([\n",
    "            \"Average\",\n",
    "            scores[model][\"average\"][\"classes\"],\n",
    "            scores[model][\"average\"][\"data_count\"],\n",
    "            scores[model][\"average\"][\"accuracy\"],\n",
    "            scores[model][\"average\"][\"macro_precision\"],\n",
    "            scores[model][\"average\"][\"macro_recall\"],\n",
    "            scores[model][\"average\"][\"macro_f1\"]\n",
    "        ])\n",
    "        print(tabulate(table, headers, tablefmt=\"grid\"))\n",
    "        print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: som\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| Dataset   |   Classes |   Test Data |   Accuracy |   Macro Precision |   Macro Recall |   Macro F1 |\n",
      "+===========+===========+=============+============+===================+================+============+\n",
      "| cifar10   |        10 |        2000 |    0.321   |          0.319399 |       0.317251 |   0.309427 |\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| mnist     |        10 |        2000 |    0.8255  |          0.830943 |       0.826743 |   0.826712 |\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| Average   |        10 |        2000 |    0.57325 |          0.575171 |       0.571997 |   0.568069 |\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "\n",
      "Model: gmm\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| Dataset   |   Classes |   Test Data |   Accuracy |   Macro Precision |   Macro Recall |   Macro F1 |\n",
      "+===========+===========+=============+============+===================+================+============+\n",
      "| cifar10   |        10 |        2000 |      0.233 |          0.193769 |       0.240086 |   0.206163 |\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| mnist     |        10 |        2000 |      0.553 |          0.477018 |       0.537317 |   0.492408 |\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| Average   |        10 |        2000 |      0.393 |          0.335394 |       0.388702 |   0.349285 |\n",
      "+-----------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "\n",
      "Model: rf\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| Dataset             |   Classes |   Test Data |   Accuracy |   Macro Precision |   Macro Recall |   Macro F1 |\n",
      "+=====================+===========+=============+============+===================+================+============+\n",
      "| abalone             |   25      |      836    |   0.252392 |          0.13397  |       0.119933 |   0.123077 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| breast_cancer       |    2      |      114    |   0.973684 |          0.971259 |       0.974423 |   0.972708 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| california_housing  |   14      |     4128    |   0.524467 |          0.373409 |       0.35493  |   0.360374 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| cifar10             |   10      |     2000    |   0.4185   |          0.413531 |       0.416781 |   0.413859 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| cifar100            |  100      |     2000    |   0.1595   |          0.160277 |       0.160221 |   0.145005 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| digits              |   10      |      360    |   0.983333 |          0.983335 |       0.983728 |   0.983215 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| dry_beans           |    6      |     1300    |   0.705385 |          0.608251 |       0.425602 |   0.463802 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| eurosat             |   10      |     2000    |   0.683    |          0.672552 |       0.66923  |   0.664415 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| fashion_mnist       |   10      |     2000    |   0.854    |          0.852403 |       0.853472 |   0.852056 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| iris                |    3      |       30    |   1        |          0.99999  |       0.99999  |   0.99994  |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| malaria             |    2      |     2000    |   0.781    |          0.781002 |       0.780988 |   0.780942 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| mnist               |   10      |     2000    |   0.9485   |          0.949244 |       0.947849 |   0.948232 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| mnist_corrupted     |   10      |     2000    |   0.948    |          0.947868 |       0.947379 |   0.947426 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| parkinsons          |    2      |       39    |   0.948718 |          0.874994 |       0.969687 |   0.912889 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| penguins            |    3      |       69    |   0.971014 |          0.96384  |       0.977773 |   0.969816 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| rice                |    2      |      762    |   0.912073 |          0.910954 |       0.909322 |   0.910046 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| rock_paper_scissors |    3      |      504    |   0.996032 |          0.995983 |       0.995959 |   0.995897 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| skin_segmentation   |    2      |     2000    |   0.9975   |          0.9946   |       0.997476 |   0.99598  |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| tf_flowers          |    5      |      734    |   0.516349 |          0.525356 |       0.50215  |   0.50138  |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| titanic             |    2      |      143    |   0.783217 |          0.778579 |       0.781586 |   0.779676 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| wine                |    3      |       36    |   1        |          0.999992 |       0.999992 |   0.999942 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| wine_quality        |    7      |     1300    |   0.674615 |          0.528937 |       0.348495 |   0.382756 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "| Average             |   10.9545 |     1197.95 |   0.774149 |          0.746378 |       0.732589 |   0.731974 |\n",
      "+---------------------+-----------+-------------+------------+-------------------+----------------+------------+\n",
      "\n",
      "Model: rf, Average Accuracy: 0.7741490989998425\n",
      "Model: dt, Average Accuracy: 0.6855864302145672\n",
      "Model: ann, Average Accuracy: 0.7433078291277186\n",
      "Model: svm, Average Accuracy: 0.7104762805783994\n",
      "Model: knn, Average Accuracy: 0.7106532517158312\n",
      "Model: som, Average Accuracy: 0.7282294990569589\n",
      "Model: gmm, Average Accuracy: 0.5570371135326512\n",
      "Model: rf, Average Image Accuracy: 0.735486724430412, Average Tabular Accuracy: 0.8009153583171404\n",
      "Model: dt, Average Image Accuracy: 0.6300319046177357, Average Tabular Accuracy: 0.7240472556277585\n",
      "Model: ann, Average Image Accuracy: 0.6995407205570694, Average Tabular Accuracy: 0.7736081350612452\n",
      "Model: svm, Average Image Accuracy: 0.6736402741134804, Average Tabular Accuracy: 0.7359781312079587\n",
      "Model: knn, Average Image Accuracy: 0.6558841745599238, Average Tabular Accuracy: 0.7485703051314593\n",
      "Model: som, Average Image Accuracy: 0.6577449480751025, Average Tabular Accuracy: 0.7770264958905517\n",
      "Model: gmm, Average Image Accuracy: 0.44606914589430485, Average Tabular Accuracy: 0.6338610911284296\n",
      "Model: rf, Average F1 Score: 0.7319741884769736\n",
      "Average Image Score: 0.7311973454593822\n",
      "Average Tabular Score: 0.7325120028737674\n",
      "Model: dt, Average F1 Score: 0.6539397960911318\n",
      "Average Image Score: 0.6282034988962923\n",
      "Average Tabular Score: 0.6717572326106359\n",
      "Model: ann, Average F1 Score: 0.7012983999159141\n",
      "Average Image Score: 0.6986895924142318\n",
      "Average Tabular Score: 0.7031044974170789\n",
      "Model: svm, Average F1 Score: 0.6571799321495155\n",
      "Average Image Score: 0.6677373037577085\n",
      "Average Tabular Score: 0.6498709825746128\n",
      "Model: knn, Average F1 Score: 0.6557265233779539\n",
      "Average Image Score: 0.63635651024775\n",
      "Average Tabular Score: 0.6691365324680951\n",
      "Model: som, Average F1 Score: 0.6837811230966795\n",
      "Average Image Score: 0.6497946330756547\n",
      "Average Tabular Score: 0.7073102315727735\n",
      "Model: gmm, Average F1 Score: 0.4439971737992437\n",
      "Average Image Score: 0.366499274274264\n",
      "Average Tabular Score: 0.497649565778076\n"
     ]
    }
   ],
   "source": [
    "models = [\"rf\"]\n",
    "# Print accuracy of som model on cifar10 dataset\n",
    "accuracies = display_model_accuracies_and_scores([\"som\"], [\"cifar10\", \"mnist\"])\n",
    "accuracies = display_model_accuracies_and_scores([\"gmm\"], [\"cifar10\", \"mnist\"])\n",
    "\n",
    "# Get all datasets by globbing the results/rf folder\n",
    "import glob\n",
    "datasets = [path.split(\"\\\\\")[-1] for path in glob.glob(\"results\\\\rf\\\\*\")]\n",
    "# Remove the .npy extension\n",
    "datasets = [dataset.split(\".\")[0] for dataset in datasets]\n",
    "\n",
    "image_datasets = [\"cifar10\", \"cifar100\", \"mnist\", \"fashion_mnist\", \"malaria\", \"mnist_corrupted\", \"rock_paper_scissors\", \"skin_segmentation\", \"tf_flowers\"]\n",
    "tabular_datasets = [dataset for dataset in datasets if dataset not in image_datasets]\n",
    "\n",
    "# Get the accuracies of the models\n",
    "accuracies = get_model_accuracies(models, datasets)\n",
    "display_model_accuracies_and_scores(models, datasets)\n",
    "\n",
    "# Get the accuracies of all the supervised models\n",
    "supervised_models = [\"rf\", \"dt\", \"ann\", \"svm\"]\n",
    "# Get the accuracies of all the unsupervised models\n",
    "unsupervised_models = [\"knn\", \"som\", \"gmm\"]\n",
    "models = supervised_models + unsupervised_models\n",
    "accuracies = get_model_accuracies(models, datasets)\n",
    "scores = get_model_scores(models, datasets)\n",
    "# Print the average accuracy of each model\n",
    "avearage_accuracies = {model: sum(accuracies[model].values()) / (len(accuracies[model])) for model in accuracies}\n",
    "for model, accuracy in avearage_accuracies.items():\n",
    "    print(f\"Model: {model}, Average Accuracy: {accuracy}\")\n",
    "\n",
    "# Print the average accuracy of each model on image vs tabular datasets\n",
    "image_accuracies = get_model_accuracies(models, image_datasets)\n",
    "tabular_accuracies = get_model_accuracies(models, tabular_datasets)\n",
    "average_image_accuracies = {model: sum(image_accuracies[model].values()) / len(image_accuracies[model]) for model in image_accuracies}\n",
    "average_tabular_accuracies = {model: sum(tabular_accuracies[model].values()) / len(tabular_accuracies[model]) for model in tabular_accuracies}\n",
    "for model, accuracy in avearage_accuracies.items():\n",
    "    print(f\"Model: {model}, Average Image Accuracy: {average_image_accuracies[model]}, Average Tabular Accuracy: {average_tabular_accuracies[model]}\")\n",
    "\n",
    "# Print the F1 scores of all the models\n",
    "average_f1_scores = {model: sum([scores[model][dataset] for dataset in datasets]) / len(datasets) for model in scores}\n",
    "for model, f1 in average_f1_scores.items():\n",
    "    print(f\"Model: {model}, Average F1 Score: {f1}\")\n",
    "    print(f\"Average Image Score: {sum([scores[model][dataset] for dataset in image_datasets]) / len(image_datasets)}\")\n",
    "    print(f\"Average Tabular Score: {sum([scores[model][dataset] for dataset in tabular_datasets]) / len(tabular_datasets)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
